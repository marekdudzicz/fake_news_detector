{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-17T15:30:07.078835Z",
     "start_time": "2020-12-17T15:30:07.076275Z"
    },
    "pycharm": {
     "name": "#%% Install library"
    }
   },
   "source": [
    "# Importing libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install catboost\n",
    "!pip install texthero\n",
    "!pip install scikit-plot\n",
    "!pip install --upgrade tables\n",
    "!pip install keras_bert\n",
    "!pip install livelossplot\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed = 0\n",
    "np.random.seed(0)\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import catboost as ctb\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from keras_bert import load_trained_model_from_checkpoint\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D,Input\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPool1D, Dropout, BatchNormalization, Bidirectional, Flatten\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import AUC, Accuracy\n",
    "import tensorflow.keras.preprocessing.text as kpt \n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from livelossplot import PlotLossesKeras\n",
    "import scikitplot as skplt\n",
    "import texthero as hero\n",
    "import joblib\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score, cross_validate, cross_val_predict\n",
    "\n",
    "from sklearn.metrics import f1_score, auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import auc, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from gensim.models import FastText, Word2Vec\n",
    "from gensim.utils import simple_preprocess, to_utf8\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title = np.load('input/train_X_title_uncased_L-24_H-1024_A-16.npy')\n",
    "X_train_text = np.load('input/train_X_text_uncased_L-24_H-1024_A-16.npy')\n",
    "\n",
    "X_test_title = np.load('input/test_X_title_uncased_L-24_H-1024_A-16.npy')\n",
    "X_test_text = np.load('input/test_X_text_uncased_L-24_H-1024_A-16.npy')\n",
    "\n",
    "test_fake = pd.read_hdf(path + '/input/test_fake.h5')\n",
    "train_fake = pd.read_hdf(path + '/input/train_fake.h5')\n",
    "y = pd.read_csv(path + '/input/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake = train_fake.fillna('empty')\n",
    "test_fake = test_fake.fillna('empty')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.max_rows', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun(x):\n",
    "    if isnan(x)==False:\n",
    "        return len(x)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake['len_text'] = train_fake['text'].map(lambda x: fun(x))\n",
    "train_fake['len_title'] = train_fake['title'].map(lambda x: fun(x))\n",
    "\n",
    "test_fake['len_text'] = test_fake['text'].map(lambda x: fun(x))\n",
    "test_fake['len_title'] = test_fake['title'].map(lambda x: fun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake['len_text'] = train_fake['text'].map(lambda x: fun(x))\n",
    "train_fake['len_title'] = train_fake['title'].map(lambda x: fun(x))\n",
    "\n",
    "test_fake['len_text'] = test_fake['text'].map(lambda x: fun(x))\n",
    "test_fake['len_title'] = test_fake['title'].map(lambda x: fun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter(train_fake, 'title_v2', 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = ' '.join([text for text in test_fake['title_v2']])\n",
    "token_phrase = token_space.tokenize(all_words)\n",
    "frequency = nltk.FreqDist(token_phrase)\n",
    "df_frequency = pd.DataFrame({\"Word\": list(frequency.keys()),\n",
    "                                \"Frequency\": list(frequency.values())})\n",
    "df_frequency = df_frequency.nlargest(columns = \"Frequency\", n = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['COVID-19', 'health-care', 'Syria', 'Aleppo', 'Syrian',\n",
    "       'elections', 'taxes', 'education', 'killed', 'The', 'immigration',\n",
    "       'Trump', 'kills', '-', 'Syrias', 'A', 'candidates-biography',\n",
    "       'civilians', 'kill', 'Damascus', 'Killed', 'economy', 'guns',\n",
    "       'Monitor', 'attack', 'federal-budget', 'Is', 'President', 'Obama',\n",
    "       'To', 'New', 'economy,jobs', 'How', 'jobs', 'Civilians',\n",
    "       'Terrorist', 'Russian', 'coronavirus', 'Can', 'In']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake = train_fake.fillna('blank')\n",
    "test_fake = test_fake.fillna('blank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_tokenizer_and_load_bert(model_name='uncased_L-2_H-128_A-2', do_lower_case=True, model_trainable=False):\n",
    "    model_dir = path + '/{}'.format(model_name)\n",
    "\n",
    "    config_path = model_dir + '/bert_config.json'\n",
    "    checkpoint_path = model_dir +'/bert_model.ckpt'\n",
    "    vocab_path = model_dir + '/vocab.txt'\n",
    "    \n",
    "    print(\"loading: {}\".format(model_name))\n",
    "    \n",
    "    tokenizer = BertTokenizer(vocab_path)\n",
    "    print(\"vocab size: {}\".format(len(tokenizer.vocab)))\n",
    "    \n",
    "    model = load_trained_model_from_checkpoint(config_path, checkpoint_path, training=model_trainable)\n",
    "    print(\"loaded: {}\".format(model_name))\n",
    "    \n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer, bert_model = init_tokenizer_and_load_bert(model_name='uncased_L-8_H-512_A-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda sent: tokenizer.encode_plus(sent, max_length=512, padding='max_length', truncation=True)\n",
    "%time train_fake['tokens_text'] = train_fake['text'].map(tokenize)\n",
    "%time train_fake['tokens_title'] = train_fake['title'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake['input_ids'] = train_fake['tokens_text'].map(lambda t: t['input_ids'] )\n",
    "train_fake['token_type_ids'] = train_fake['tokens_text'].map(lambda t: t['token_type_ids'] )\n",
    "train_fake['attention_mask'] = train_fake['tokens_text'].map(lambda t: t['attention_mask'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake['input_ids_title'] = train_fake['tokens_title'].map(lambda t: t['input_ids'] )\n",
    "train_fake['token_type_ids_title'] = train_fake['tokens_title'].map(lambda t: t['token_type_ids'] )\n",
    "train_fake['attention_mask_title'] = train_fake['tokens_title'].map(lambda t: t['attention_mask'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = np.stack(train_fake['input_ids'])\n",
    "token_type_ids = np.stack(train_fake['token_type_ids'])\n",
    "attention_mask = np.stack(train_fake['attention_mask'])\n",
    "\n",
    "input_ids.shape, token_type_ids.shape, attention_mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_title = np.stack(train_fake['input_ids_title'])\n",
    "token_type_ids_title = np.stack(train_fake['token_type_ids_title'])\n",
    "attention_mask_title = np.stack(train_fake['attention_mask_title'])\n",
    "\n",
    "input_ids_title.shape, token_type_ids_title.shape, attention_mask_title.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time predicts = bert_model.predict([input_ids, token_type_ids, attention_mask], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time predicts_title = bert_model.predict([input_ids_title, token_type_ids_title, attention_mask_title], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = predicts[:, 0 , :]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = X_train.columns[1:]\n",
    "X_train = X_train[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['is_fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(max_depth=3, n_estimators=30, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=3, n_estimators=50, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ctb.CatBoostClassifier(max_depth=3, n_estimators=50, verbose=0, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "asd = train_fake['len_text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_title, X_train_text), axis = 1)\n",
    "X_test = np.concatenate((X_test_title, X_test_text), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df = pd.DataFrame(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_df['text_len'] = train_fake['len_text']\n",
    "X_train_df['title_len'] = train_fake['len_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y['is_fake']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(max_depth = 3, n_estimators=20, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train_df, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.XGBClassifier(max_depth=3, n_estimators=40, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.estimators.plot_learning_curve(model, X_train_df, y, figsize = (7, 4), cv = 3, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = test_fake.columns[1:]\n",
    "test_fake_1 = test_fake[feats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(max_depth=3, n_estimators=40, random_state=0)\n",
    "%time scores = cross_val_score(model, X_train, y, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_fake_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake['is_fake'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake[ ['id', 'is_fake'] ].to_csv('bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x, word):\n",
    "    if word in x:\n",
    "        return 1;\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    train_fake['title'].map(lambda x: g(x,word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.concatenate((X_train_title, X_train_text), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.concatenate((X_test_title, X_test_text), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Dense(1400, input_dim = 2048, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.8),\n",
    "\n",
    "\n",
    "    Dense(600, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(200, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.7),\n",
    "\n",
    "    Dense(40, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.6),\n",
    "\n",
    "    Dense(2, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['AUC'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks1 = [ \n",
    "    EarlyStopping(monitor = 'loss', patience = 7), \n",
    "    ReduceLROnPlateau(monitor = 'loss', patience = 3), \n",
    "    ModelCheckpoint('../model.best.hdf5', save_best_only=True) # saving the best model\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1 = X_train[:4000]\n",
    "X_2 = X_train[4000:]\n",
    "y_1 = to_categorical(y[:4000])\n",
    "y_2 = to_categorical(y[4000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_1.shape, y_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_2.shape, y_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_history = model.fit(X_1, y_1,\n",
    "          batch_size = 128, epochs = 100, verbose = 1,\n",
    "          callbacks = callbacks1,\n",
    "          validation_data = (X_2, y_2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('../model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_learning_curve(history, keys=['auc', 'loss']):\n",
    "    plt.figure(figsize=(20,8))\n",
    "    for i, key in enumerate(keys):\n",
    "        plt.subplot(1, 2, i + 1)\n",
    "        sns.lineplot(x = history.epoch, y = history.history[key])\n",
    "        sns.lineplot(x = history.epoch, y = history.history['val_' + key])\n",
    "        plt.title('Learning Curve')\n",
    "        plt.ylabel(key.title())\n",
    "        plt.xlabel('Epoch')\n",
    "#         plt.ylim(ylim)\n",
    "        plt.legend(['train', 'test'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curve(learning_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_2, y_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isnan(value):\n",
    "    try:\n",
    "        return math.isnan(float(value))\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake['len_text'] = train_fake['text'].map(lambda x: fun(x))\n",
    "train_fake['len_title'] = train_fake['title'].map(lambda x: fun(x))\n",
    "\n",
    "test_fake['len_text'] = test_fake['text'].map(lambda x: fun(x))\n",
    "test_fake['len_title'] = test_fake['title'].map(lambda x: fun(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake.sample(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_topn(text):\n",
    "    top_tokens = [ x for x in words ]\n",
    "    \n",
    "    def check_top_n(sent_tokens):\n",
    "        return [int(token in sent_tokens) for token in top_tokens]\n",
    "\n",
    "    df_topn = text.str.split(\" \").map(set).map(check_top_n).apply(pd.Series)\n",
    "    df_topn.columns = top_tokens\n",
    "\n",
    "    return df_topn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_topn(train_fake['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_v2 = train_fake['is_fake']\n",
    "model_v1 = lgb.LGBMClassifier(max_depth = 3, n_estimators=20, random_state=0)\n",
    "%time scores = cross_val_score(model_v1, df, y_v2, cv=3, scoring='roc_auc')\n",
    "\n",
    "np.mean(scores), np.std(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skplt.estimators.plot_learning_curve(model_v1, df, y_v2, figsize = (7, 4), cv = 3, scoring = 'roc_auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_v1.fit(df, y_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test_fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_df_topn(test_fake['title'])\n",
    "df['y_pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = model_v1.predict(df)\n",
    "test_fake['is_fake'] = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake[ ['id', 'is_fake'] ].to_csv('bert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fake[ ['id', 'is_fake'] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_X_2 = model.predict(X_2)\n",
    "y_pred_X_2 = np.argmax(y_pred_X_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2_pred = np.argmax(y_2, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_2_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_pred_X_2, y_2_pred)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}